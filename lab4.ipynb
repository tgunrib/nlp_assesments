{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e610b059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tosin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sys, codecs, json, math, time, warnings\n",
    "import sklearn_crfsuite\n",
    "import sklearn\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "warnings.simplefilter( action='ignore', category=FutureWarning )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0395ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset( dataset_file, max_files = 50 ) :\n",
    "    # load parsed ontonotes dataset\n",
    "    readHandle = codecs.open( dataset_file, 'r', 'utf-8', errors = 'replace' )\n",
    "    str_json = readHandle.read()\n",
    "    readHandle.close()\n",
    "    dict_ontonotes = json.loads( str_json )\n",
    "\n",
    "    # make a training and test split\n",
    "    list_files = list( dict_ontonotes.keys() )\n",
    "    if len(list_files) > max_files :\n",
    "        list_files = list_files[ :max_files ]\n",
    "    nSplit = math.floor( len(list_files)*0.99 )\n",
    "    list_train_files = list_files[ : nSplit ]\n",
    "    list_test_files = list_files[ nSplit : ]\n",
    "\n",
    "    # sent = (tokens, pos, IOB_label)\n",
    "    list_train = []\n",
    "    for str_file in list_train_files :\n",
    "        for str_sent_index in dict_ontonotes[str_file] :\n",
    "            # ignore sents with non-PENN POS tags\n",
    "            if 'XX' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "                continue\n",
    "            if 'VERB' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "                continue\n",
    "\n",
    "            list_entry = []\n",
    "\n",
    "            # compute IOB tags for named entities (if any)\n",
    "            ne_type_last = None\n",
    "            for nTokenIndex in range(len(dict_ontonotes[str_file][str_sent_index]['tokens'])) :\n",
    "                strToken = dict_ontonotes[str_file][str_sent_index]['tokens'][nTokenIndex]\n",
    "                strPOS = dict_ontonotes[str_file][str_sent_index]['pos'][nTokenIndex]\n",
    "                ne_type = None\n",
    "                if 'ne' in dict_ontonotes[str_file][str_sent_index] :\n",
    "                    dict_ne = dict_ontonotes[str_file][str_sent_index]['ne']\n",
    "                    if not 'parse_error' in dict_ne :\n",
    "                        for str_NEIndex in dict_ne :\n",
    "                            if nTokenIndex in dict_ne[str_NEIndex]['tokens'] :\n",
    "                                ne_type = dict_ne[str_NEIndex]['type']\n",
    "                                break\n",
    "                if ne_type != None :\n",
    "                    if ne_type == ne_type_last :\n",
    "                        strIOB = 'I-' + ne_type\n",
    "                    else :\n",
    "                        strIOB = 'B-' + ne_type\n",
    "                else :\n",
    "                    strIOB = 'O'\n",
    "                ne_type_last = ne_type\n",
    "                \n",
    "                list_entry.append( ( strToken, strPOS, strIOB ) )\n",
    "\n",
    "            list_train.append( list_entry )\n",
    "\n",
    "    list_test = []\n",
    "    for str_file in list_test_files :\n",
    "        for str_sent_index in dict_ontonotes[str_file] :\n",
    "\t\t\t# ignore sents with non-PENN POS tags\n",
    "            if 'XX' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "                continue\n",
    "            if 'VERB' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "                continue\n",
    "\n",
    "            list_entry = []\n",
    "\n",
    "            #compute IOB tags for named entities (if any)\n",
    "            ne_type_last = None\n",
    "            for nTokenIndex in range(len(dict_ontonotes[str_file][str_sent_index]['tokens'])) :\n",
    "                strToken = dict_ontonotes[str_file][str_sent_index]['tokens'][nTokenIndex]\n",
    "                strPOS = dict_ontonotes[str_file][str_sent_index]['pos'][nTokenIndex]\n",
    "                ne_type = None\n",
    "                if 'ne' in dict_ontonotes[str_file][str_sent_index] :\n",
    "                    dict_ne = dict_ontonotes[str_file][str_sent_index]['ne']\n",
    "                    if not 'parse_error' in dict_ne :\n",
    "                        for str_NEIndex in dict_ne :\n",
    "                            if nTokenIndex in dict_ne[str_NEIndex]['tokens'] :\n",
    "                                ne_type = dict_ne[str_NEIndex]['type']\n",
    "                                break\n",
    "                if ne_type != None:\n",
    "                    if ne_type == ne_type_last :\n",
    "                        strIOB = 'I-' + ne_type\n",
    "                    else :\n",
    "                        strIOB = 'B-' + ne_type\n",
    "                else :\n",
    "                    strIOB = 'O'\n",
    "                ne_type_last = ne_type\n",
    "                \n",
    "                list_entry.append( ( strToken, strPOS, strIOB ) )\n",
    "            list_test.append( list_entry )\n",
    "        return list_train, list_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d52beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def names(check):\n",
    "    names_possibility = \"((?:Dr|Mr|Mrs|Miss|Sir|Lord|Lady|King|Professor|Doctor|Madam|Gentleman|Dame)\\.?\\s*(?:[A-Z]\\.?\\s?)*(?:[A-Z][a-z0-9\\-\\.]+\\s?)+|(?:[A-Z]\\.\\s?)+(?:[A-Z][a-z0-9\\-\\.]+\\s?)+)\"\n",
    "    result = re.findall(names_possibility, check)\n",
    "    result = [re.sub('\\r|\\n|\\'|\"|“|’', '', word.strip().strip('.').lower(), re.IGNORECASE) for word in result]\n",
    "    return result\n",
    "\n",
    "def names_match(text):\n",
    "    names_possibility = \"((?:Dr|Mr|Mrs|Miss|Sir|Lord|Lady|King|Professor|Doctor|Madam|Gentleman|Dame)\\.?\\s*(?:[A-Z]\\.?\\s?)*(?:[A-Z][a-z0-9\\-\\.]+\\s?)+|(?:[A-Z]\\.\\s?)+(?:[A-Z][a-z0-9\\-\\.]+\\s?)+)\"\n",
    "    return re.findall(names_possibility, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086f694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_textfile(filename):\n",
    "    text = ''\n",
    "    for line in codecs.open(filename, \"r\", encoding=\"utf-8\"):\n",
    "        text += line\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_pos_tags = [nltk.pos_tag(nltk.word_tokenize(word)) for word in sentences]\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    # stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    word_lemmas = []\n",
    "    sents = names(text)\n",
    "    for sent in word_pos_tags:\n",
    "        # word_lemmas.append([(stemmer.stem(word[0]), word[1]) for word in sent])\n",
    "        word_lemmas.append([(lemmatizer.lemmatize(word[0]), word[1]) for word in sent])\n",
    "    return word_pos_tags, sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8beafbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_word2features(sent, i):\n",
    "\tword = sent[i][0]\n",
    "\tpostag = sent[i][1]\n",
    "\n",
    "\tfeatures = {\n",
    "\t\t'word' : word,\n",
    "\t\t'postag': postag,\n",
    "\n",
    "\t\t# token shape\n",
    "\t\t'word.lower()': word.lower(),\n",
    "\t\t'word.isupper()': word.isupper(),\n",
    "\t\t'word.istitle()': word.istitle(),\n",
    "\t\t'word.isdigit()': word.isdigit(),\n",
    "\t\t# token suffix\n",
    "\t\t'word.suffix': word.lower()[-3:],\n",
    "\n",
    "\t\t# POS prefix\n",
    "\t\t'postag[:2]': postag[:2],\n",
    "\t}\n",
    "\tif i > 0:\n",
    "\t\tword_prev = sent[i-1][0]\n",
    "\t\tpostag_prev = sent[i-1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'-1:word.lower()': word_prev.lower(),\n",
    "\t\t\t'-1:postag': postag_prev,\n",
    "\t\t\t'-1:word.lower()': word_prev.lower(),\n",
    "\t\t\t'-1:word.isupper()': word_prev.isupper(),\n",
    "\t\t\t'-1:word.istitle()': word_prev.istitle(),\n",
    "\t\t\t'-1:word.isdigit()': word_prev.isdigit(),\n",
    "\t\t\t'-1:word.suffix': word_prev.lower()[-3:],\n",
    "\t\t\t'-1:postag[:2]': postag_prev[:2],\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['BOS'] = True\n",
    "\n",
    "\tif i < len(sent)-1:\n",
    "\t\tword_next = sent[i+1][0]\n",
    "\t\tpostag_next = sent[i+1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'+1:word.lower()': word_next.lower(),\n",
    "\t\t\t'+1:postag': postag_next,\n",
    "\t\t\t'+1:word.lower()': word_next.lower(),\n",
    "\t\t\t'+1:word.isupper()': word_next.isupper(),\n",
    "\t\t\t'+1:word.istitle()': word_next.istitle(),\n",
    "\t\t\t'+1:word.isdigit()': word_next.isdigit(),\n",
    "\t\t\t'+1:word.suffix': word_next.lower()[-3:],\n",
    "\t\t\t'+1:postag[:2]': postag_next[:2],\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['EOS'] = True\n",
    "\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37797c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent, word2features_func = None):\n",
    "\treturn [word2features_func(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "\treturn [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "\treturn [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6c3b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_task(filebook, dataset_file, word2features_func, max_files = 20,train_crf_model_func = None,  max_iter = 50) :\n",
    "    #make a dataset from english NE labelled ontonotes sents\n",
    "    train_sents, test_sents = create_dataset( dataset_file, max_files = max_files )\n",
    "    txt_sents, sentences = preprocess_textfile(filebook)\n",
    "    \n",
    "   \n",
    "    #sent = names(text)\n",
    "    #word_pos_tags = [nltk.pos_tag(nltk.word_tokenize(word)) for word in sent]\n",
    "    \n",
    "\n",
    "    # create feature vectors for every sent\n",
    "    X_train = [sent2features(s, word2features_func = word2features_func) for s in train_sents]\n",
    "    Y_train = [sent2labels(s) for s in train_sents]\n",
    "    \n",
    "    unsup_text = [sent2features(s, word2features_func = word2features_func) for s in txt_sents]\n",
    "    #name_text = [sent2features(s, word2features_func = word2features_func) for s in word_pos_tags]\n",
    "\n",
    "    X_test = [sent2features(s, word2features_func = word2features_func) for s in test_sents]\n",
    "    Y_test = [sent2labels(s) for s in test_sents]\n",
    "    \n",
    "\n",
    "    vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(sublinear_tf=True, encoding='utf-8', decode_error='ignore', lowercase=False)\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "    chap_vectorized = vectorizer.transform(unsup_text)\n",
    "\n",
    "    # get the label set\n",
    "    set_labels = set([])\n",
    "    for data in [Y_train,Y_test] :\n",
    "        for n_sent in range(len(data)) :\n",
    "            for str_label in data[n_sent] :\n",
    "                set_labels.add( str_label )\n",
    "    labels = list( set_labels )\n",
    "\n",
    "    # remove 'O' label as we are not usually interested in how well 'O' is predicted\n",
    "    #labels = list( crf.classes_ )\n",
    "    labels.remove('O')\n",
    "    \n",
    "    \n",
    "    crf = train_crf_model_func( X_train_vectorized, Y_train, max_iter, labels)\n",
    "    Y_pred = crf.predict( X_test_vectorized )\n",
    "    #names_pred = crf.predict(name_text)\n",
    "    result_pred = crf.predict(chap_vectorized)\n",
    "    #print(len(Y_pred[1]), len(X_test[1]))\n",
    "    sorted_labels = sorted(\n",
    "        labels, \n",
    "        key=lambda name: (name[1:], name[0])\n",
    "    )\n",
    "     \n",
    "    macro_scores = sklearn_crfsuite.metrics.flat_classification_report( Y_test, Y_pred, labels=sorted_labels)\n",
    "    print( macro_scores )\n",
    "    result = []\n",
    "    accept = ['PERSON']\n",
    "    for i in range(0,len(result_pred)):\n",
    "        conlltags = [(word['word'], word['postag'] , tg) for tg, word in zip(result_pred[i], unsup_text[i])]\n",
    "        ne_tree = nltk.chunk.conlltags2tree(conlltags)\n",
    "        for subtree in ne_tree:\n",
    "            if type(subtree) == nltk.tree.Tree:\n",
    "                original_label = subtree.label()\n",
    "                original_string = \" \".join([token for token, pos in subtree.leaves()]).lower().strip()\n",
    "                result.append((original_string, original_label))\n",
    "    \"\"\"for i in range(0,len(names_pred)):\n",
    "        conlltags = [(word['word'], word['postag'] , tg) for tg, word in zip(names_pred[i], name_text[i])]\n",
    "        ne_tree = nltk.chunk.conlltags2tree(conlltags)\n",
    "        for subtree in ne_tree:\n",
    "            if type(subtree) == nltk.tree.Tree:\n",
    "                original_label = subtree.label()\n",
    "                original_string = \" \".join([token for token, pos in subtree.leaves()]).lower().strip()\n",
    "                result.append((original_string, original_label))\"\"\"\n",
    "    d = []\n",
    "    for value,key in result:\n",
    "        value = re.sub('\\r|\\n|\\'|\"|“|’', '', value.strip(), re.IGNORECASE)\n",
    "        if value not in d:\n",
    "            d.append(value)\n",
    "    \n",
    "    d = d + sentences\n",
    "    return set(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e7395bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_task(filebook, dataset_file, word2features_func, max_files=10, train_crf_model_func=None, max_iter=100):\n",
    "    # make a dataset from english NE labelled ontonotes sents\n",
    "    train_sents, test_sents = create_dataset(dataset_file, max_files=max_files)\n",
    "    txt_sents, sentences = preprocess_textfile(filebook)\n",
    "    #tagged_words = [(word, tag) for word, tag in train_sents]\n",
    "\n",
    "    \"\"\"fd = nltk.FreqDist(tagged_words)\n",
    "    smoothing_factor = 1  # Add-one smoothing\n",
    "    lpd = LidstoneProbDist(fd, smoothing_factor)\n",
    "    for word_pos_tuple in tagged_words:\n",
    "        word, pos_tag = word_pos_tuple\n",
    "        smoothed_prob = lpd.prob(word_pos_tuple)\n",
    "        word_pos_tuple = (word,pos_tag,smoothed_prob)\"\"\"\n",
    "\n",
    "    # create feature vectors for every sent\n",
    "    X_train = [sent2features(s, word2features_func=word2features_func) for s in train_sents]\n",
    "    Y_train = [sent2labels(s) for s in train_sents]\n",
    "    unsup_text = [sent2features(s, word2features_func=word2features_func) for s in txt_sents]\n",
    "\n",
    "\n",
    "\n",
    "    X_test = [sent2features(s, word2features_func=word2features_func) for s in test_sents]\n",
    "    Y_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "    vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(sublinear_tf=True, encoding='utf-8', decode_error='ignore', lowercase=False) \n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "    chap_vectorized = vectorizer.transform(unsup_text)\n",
    "    \n",
    "    # get the label set\n",
    "    set_labels = set([])\n",
    "    for data in [Y_train, Y_test]:\n",
    "        for n_sent in range(len(data)):\n",
    "            for str_label in data[n_sent]:\n",
    "                set_labels.add(str_label)\n",
    "    labels = list(set_labels)\n",
    "\n",
    "    # remove 'O' label as we are not usually interested in how well 'O' is predicted\n",
    "    # labels = list( crf.classes_ )\n",
    "    labels.remove('O')\n",
    "    \n",
    "    crf = train_crf_model_func( X_train_vectorized, Y_train, max_iter, labels)\n",
    "    Y_pred = crf.predict( X_test_vectorized )\n",
    "    #names_pred = crf.predict(name_text)\n",
    "    result_pred = crf.predict(chap_vectorized)\n",
    "    #print(len(Y_pred[1]), len(X_test[1]))\n",
    "    sorted_labels = sorted(\n",
    "        labels, \n",
    "        key=lambda name: (name[1:], name[0])\n",
    "    )\n",
    "\n",
    "    # macro_scores = sklearn_crfsuite.metrics.flat_classification_report( Y_test, Y_pred, labels=sorted_labels)\n",
    "    # print( macro_scores )\n",
    "    result = []\n",
    "    # print(len(X_test[1]), len(Y_pred[1]))\n",
    "    for i in range(0, len(result_pred)):\n",
    "        conlltags = [(word['word'], word['postag'], tg) for tg, word in zip(result_pred[i], unsup_text[i])]\n",
    "        ne_tree = nltk.chunk.conlltags2tree(conlltags)\n",
    "        for subtree in ne_tree:\n",
    "            if type(subtree) == nltk.tree.Tree:\n",
    "                original_label = subtree.label()\n",
    "                original_string = \" \".join([token for token, pos in subtree.leaves()]).lower().strip()\n",
    "                result.append((original_string, original_label))\n",
    "    d = {}\n",
    "    for value, key in result:\n",
    "        value = re.sub('[\\r|\\n|\\'|\"|“|’|\\t]+', '', value.strip(), re.IGNORECASE)\n",
    "        if key in d and value not in d[key] or key not in d:\n",
    "            d.setdefault(key, []).append(value)\n",
    "            \n",
    "    for value in sentences:\n",
    "        value = re.sub('[\\r|\\n|\\'|\"|“|’|\\t]+', '', value.strip(), re.IGNORECASE)\n",
    "        if 'PERSON' in d and value not in d['PERSON'] or 'PERSON' not in d:\n",
    "            d.setdefault('PERSON', []).append(value)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b54fb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn_crfsuite import metrics\n",
    "def task5_train_crf_model( X_train, Y_train, max_iter, labels ) :\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "      algorithm='lbfgs',\n",
    "      max_iterations=100,\n",
    "      all_possible_transitions=True\n",
    "  )\n",
    "    params_space = {\n",
    "      'c1': scipy.stats.expon(scale=0.5),\n",
    "      'c2': scipy.stats.expon(scale=0.05),\n",
    "  }\n",
    "\n",
    "  # use the same metric for evaluation\n",
    "    f1_scorer = make_scorer(sklearn_crfsuite.metrics.flat_f1_score,\n",
    "                          average='weighted', labels=labels)\n",
    "\n",
    "  # search\n",
    "    rs = sklearn.model_selection.RandomizedSearchCV(crf, params_space,\n",
    "                          cv=3,\n",
    "                          verbose=1,\n",
    "                          n_jobs=-1,\n",
    "                          n_iter=10,\n",
    "                          scoring=f1_scorer)\n",
    "    rs.fit(X_train, Y_train)\n",
    "    print('best params:', rs.best_params_)\n",
    "    print('best CV score:', rs.best_score_)\n",
    "    print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f25038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task3_train_crf_model( X_train, Y_train, max_iter, labels ) :\n",
    "\t# train the basic CRF model\n",
    "\tcrf = sklearn_crfsuite.CRF(\n",
    "\t\talgorithm='lbfgs',\n",
    "\t\tc1=0.15632504647140685,\n",
    "\t\tc2=0.043957202811694684,\n",
    "\t\tmax_iterations=max_iter,\n",
    "\t\tall_possible_transitions=True,\n",
    "\t)\n",
    "\tcrf.fit(X_train, Y_train)\n",
    "\treturn crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "150034a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-db908a4afbd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_book\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\\\Users\\\\tosin\\\\Documents\\\\Comp3225_coursework\\\\comp3225_example_package\\\\eval_chapter.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0montonotes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\\\Users\\\\tosin\\\\Documents\\\\Comp3225_coursework\\\\comp3225_example_package\\\\ontonotes_parsed.json'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mexec_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_book\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0montonotes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2features_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask2_word2features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_crf_model_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask3_train_crf_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-c1ce431dbefa>\u001b[0m in \u001b[0;36mexec_task\u001b[1;34m(filebook, dataset_file, word2features_func, max_files, train_crf_model_func, max_iter)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mX_train_vectorized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mX_test_vectorized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mchap_vectorized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munsup_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\comp3225_cwk\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1839\u001b[0m         \"\"\"\n\u001b[0;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1841\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\comp3225_cwk\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\comp3225_cwk\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\comp3225_cwk\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "file_book = 'C:\\\\Users\\\\tosin\\\\Documents\\\\Comp3225_coursework\\\\comp3225_example_package\\\\eval_chapter.txt'\n",
    "ontonotes = 'C:\\\\Users\\\\tosin\\\\Documents\\\\Comp3225_coursework\\\\comp3225_example_package\\\\ontonotes_parsed.json'\n",
    "exec_task(file_book, ontonotes, word2features_func = task2_word2features,train_crf_model_func = task3_train_crf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7f3c046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mrs.creakle', 'mr. mell', 'mr. creakle', 'j. steerforth', 'mrs. mell', 'mrs. creakle', 'j. steerforth. steerforth', 'mr. sharp', 'mr. mell. mr. mell', 'mr.creakle', 'b. smith', 'miss creakle'}\n"
     ]
    }
   ],
   "source": [
    "text = 'Rachel B. Smith'\n",
    "for line in codecs.open(file_book, \"r\", encoding=\"utf-8\"):\n",
    "    text += line\n",
    "print(set(names(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d37a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
