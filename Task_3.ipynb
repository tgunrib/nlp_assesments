{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2637cf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tosin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sys, codecs, json, math, time, warnings\n",
    "import sklearn_crfsuite\n",
    "import sklearn\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "warnings.simplefilter( action='ignore', category=FutureWarning )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27649772",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_book = 'C:\\\\Users\\\\tosin\\\\Documents\\\\Comp3225_coursework\\\\books_nlp\\\\book_1.txt'\n",
    "ontonotes = 'C:\\\\Users\\\\tosin\\\\Documents\\\\Comp3225_coursework\\\\comp3225_example_package\\\\ontonotes_parsed.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45871e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_textfile(filename):\n",
    "    text = ''\n",
    "    for line in codecs.open(file_book, \"r\", encoding=\"utf-8\"):\n",
    "        text += line\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_pos_tags = [nltk.pos_tag(nltk.word_tokenize(word)) for word in sentences]\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    word_lemmas = []\n",
    "    for sent in word_pos_tags:\n",
    "        word_lemmas.append([(stemmer.stem(word[0]), word[1]) for word in sent])\n",
    "       # word_lemmas.append([(lemmatizer.lemmatize(word[0]), word[1]) for word in sent])\n",
    "    return word_pos_tags, word_pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a7d6db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_word2features(sent, i):\n",
    "\tword = sent[i][0]\n",
    "\tpostag = sent[i][1]\n",
    "\n",
    "\tfeatures = {\n",
    "\t\t'word' : word,\n",
    "\t\t'postag': postag,\n",
    "\n",
    "\t\t# token shape\n",
    "\t\t'word.lower()': word.lower(),\n",
    "\t\t'word.isupper()': word.isupper(),\n",
    "\t\t'word.istitle()': word.istitle(),\n",
    "\t\t'word.isdigit()': word.isdigit(),\n",
    "\n",
    "\t\t# token suffix\n",
    "\t\t'word.suffix': word.lower()[-3:],\n",
    "\n",
    "\t\t# POS prefix\n",
    "\t\t'postag[:2]': postag[:2],\n",
    "\t}\n",
    "\tif i > 0:\n",
    "\t\tword_prev = sent[i-1][0]\n",
    "\t\tpostag_prev = sent[i-1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'-1:word.lower()': word_prev.lower(),\n",
    "\t\t\t'-1:postag': postag_prev,\n",
    "\t\t\t'-1:word.lower()': word_prev.lower(),\n",
    "\t\t\t'-1:word.isupper()': word_prev.isupper(),\n",
    "\t\t\t'-1:word.istitle()': word_prev.istitle(),\n",
    "\t\t\t'-1:word.isdigit()': word_prev.isdigit(),\n",
    "\t\t\t'-1:word.suffix': word_prev.lower()[-3:],\n",
    "\t\t\t'-1:postag[:2]': postag_prev[:2],\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['BOS'] = True\n",
    "\n",
    "\tif i < len(sent)-1:\n",
    "\t\tword_next = sent[i+1][0]\n",
    "\t\tpostag_next = sent[i+1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'+1:word.lower()': word_next.lower(),\n",
    "\t\t\t'+1:postag': postag_next,\n",
    "\t\t\t'+1:word.lower()': word_next.lower(),\n",
    "\t\t\t'+1:word.isupper()': word_next.isupper(),\n",
    "\t\t\t'+1:word.istitle()': word_next.istitle(),\n",
    "\t\t\t'+1:word.isdigit()': word_next.isdigit(),\n",
    "\t\t\t'+1:word.suffix': word_next.lower()[-3:],\n",
    "\t\t\t'+1:postag[:2]': postag_next[:2],\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['EOS'] = True\n",
    "\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bdd91ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_word2features(sent, i):\n",
    "\n",
    "\tword = sent[i][0]\n",
    "\tpostag = sent[i][1]\n",
    "\n",
    "\tfeatures = {\n",
    "\t\t# basic features - token and POS tag\n",
    "\t\t'word' : word,\n",
    "\t\t'postag': postag,\n",
    "\t}\n",
    "\tif i > 0:\n",
    "\t\t# features for previous word (context)\n",
    "\t\tword_prev = sent[i-1][0]\n",
    "\t\tpostag_prev = sent[i-1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'-1:word.lower()': word_prev.lower(),\n",
    "\t\t\t'-1:postag': postag_prev,\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['BOS'] = True\n",
    "\n",
    "\tif i < len(sent)-1:\n",
    "\t\t# features for next word (context)\n",
    "\t\tword_next = sent[i+1][0]\n",
    "\t\tpostag_next = sent[i+1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'+1:word.lower()': word_next.lower(),\n",
    "\t\t\t'+1:postag': postag_next,\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['EOS'] = True\n",
    "\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a7240f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0, \n",
    "        'word.lower()': word.lower(), \n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2451782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent, word2features_func = None):\n",
    "\treturn [word2features_func(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "\treturn [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "\treturn [token for token, postag, label in sent]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b62f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_task(filebook, dataset_file, word2features_func, max_files = 20,train_crf_model_func = None,  max_iter = 50) :\n",
    "    #make a dataset from english NE labelled ontonotes sents\n",
    "    train_sents, test_sents = create_dataset( dataset_file, max_files = max_files )\n",
    "    txt_sents, sentences = preprocess_textfile(file_book)\n",
    "    \n",
    "\n",
    "    # create feature vectors for every sent\n",
    "    X_train = [sent2features(s, word2features_func = word2features_func) for s in train_sents]\n",
    "    Y_train = [sent2labels(s) for s in train_sents]\n",
    "    unsup_text = [sent2features(s, word2features_func = word2features_func) for s in txt_sents]\n",
    "\n",
    "    X_test = [sent2features(s, word2features_func = word2features_func) for s in test_sents]\n",
    "    Y_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "    # get the label set\n",
    "    set_labels = set([])\n",
    "    for data in [Y_train,Y_test] :\n",
    "        for n_sent in range(len(data)) :\n",
    "            for str_label in data[n_sent] :\n",
    "                set_labels.add( str_label )\n",
    "    labels = list( set_labels )\n",
    "\n",
    "    # remove 'O' label as we are not usually interested in how well 'O' is predicted\n",
    "    #labels = list( crf.classes_ )\n",
    "    labels.remove('O')\n",
    "    \n",
    "\n",
    "    crf = train_crf_model_func( X_train, Y_train, max_iter, labels)\n",
    "    Y_pred = crf.predict( X_test )\n",
    "    result_pred = crf.predict(unsup_text)\n",
    "    #print(len(Y_pred[1]), len(X_test[1]))\n",
    "    sorted_labels = sorted(\n",
    "        labels, \n",
    "        key=lambda name: (name[1:], name[0])\n",
    "    )\n",
    "     \n",
    "    macro_scores = sklearn_crfsuite.metrics.flat_classification_report( Y_test, Y_pred, labels=sorted_labels)\n",
    "    print( macro_scores )\n",
    "    result = []\n",
    "    accept = ['DATE','CARDINAL','ORDINAL','NORP']\n",
    "    #print(len(X_test[1]), len(Y_pred[1]))\n",
    "    for i in range(0,len(Y_pred)):\n",
    "        conlltags = [(word['word'], word['postag'] , tg) for tg, word in zip(result_pred[i], unsup_text[i])]\n",
    "        ne_tree = nltk.chunk.conlltags2tree(conlltags)\n",
    "        for subtree in ne_tree:\n",
    "            if type(subtree) == nltk.tree.Tree:\n",
    "                original_label = subtree.label()\n",
    "                original_string = \" \".join([token for token, pos in subtree.leaves()]).lower().strip()\n",
    "                result.append((original_string, original_label))\n",
    "    d = {}\n",
    "    for value,key in result:\n",
    "        d.setdefault(key, []).append(value)\n",
    "    return d\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39bfb7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn_crfsuite import metrics\n",
    "def task1_train_crf_model( X_train, Y_train, max_iter, labels ) :\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "      algorithm='lbfgs',\n",
    "      max_iterations=100,\n",
    "      all_possible_transitions=True\n",
    "  )\n",
    "    params_space = {\n",
    "      'c1': scipy.stats.expon(scale=0.05),\n",
    "      'c2': scipy.stats.expon(scale=0.5),\n",
    "  }\n",
    "    \n",
    "  # use the same metric for evaluation\n",
    "    f1_scorer = make_scorer(sklearn_crfsuite.metrics.flat_f1_score,\n",
    "                          average='weighted', labels=labels)\n",
    "  # search\n",
    "    rs = sklearn.model_selection.RandomizedSearchCV(crf, params_space,\n",
    "                          cv=3,\n",
    "                          verbose=1,\n",
    "                          n_jobs=-1,\n",
    "                          n_iter=30,\n",
    "                          scoring=f1_scorer)\n",
    "    rs.fit(X_train, Y_train)\n",
    "    print('best params:', rs.best_params_)\n",
    "    print('best CV score:', rs.best_score_)\n",
    "    print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a286a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task3_train_crf_model( X_train, Y_train, max_iter, labels ) :\n",
    "\t# train the basic CRF model\n",
    "\tcrf = sklearn_crfsuite.CRF(\n",
    "\t\talgorithm='lbfgs',\n",
    "\t\tc1=0.1,\n",
    "\t\tc2=0.1,\n",
    "\t\tmax_iterations=max_iter,\n",
    "\t\tall_possible_transitions=True,\n",
    "\t)\n",
    "\tcrf.fit(X_train, Y_train)\n",
    "\treturn crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1729ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset( dataset_file, max_files = 50 ) :\n",
    "\t# load parsed ontonotes dataset\n",
    "\treadHandle = codecs.open( dataset_file, 'r', 'utf-8', errors = 'replace' )\n",
    "\tstr_json = readHandle.read()\n",
    "\treadHandle.close()\n",
    "\tdict_ontonotes = json.loads( str_json )\n",
    "    \n",
    "\t# make a training and test split\n",
    "\tlist_files = list( dict_ontonotes.keys() )\n",
    "\tif len(list_files) > max_files :\n",
    "\t\tlist_files = list_files[ :max_files ]\n",
    "\tnSplit = math.floor( len(list_files)*0.9 )\n",
    "\tlist_train_files = list_files[ : nSplit ]\n",
    "\tlist_test_files = list_files[ nSplit : ]\n",
    "\n",
    "\t# sent = (tokens, pos, IOB_label)\n",
    "\tlist_train = []\n",
    "\tfor str_file in list_train_files :\n",
    "\t\tfor str_sent_index in dict_ontonotes[str_file] :\n",
    "\t\t\t# ignore sents with non-PENN POS tags\n",
    "\t\t\tif 'XX' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif 'VERB' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tlist_entry = []\n",
    "\n",
    "\t\t\t# compute IOB tags for named entities (if any)\n",
    "\t\t\tne_type_last = None\n",
    "\t\t\tfor nTokenIndex in range(len(dict_ontonotes[str_file][str_sent_index]['tokens'])) :\n",
    "\t\t\t\tstrToken = dict_ontonotes[str_file][str_sent_index]['tokens'][nTokenIndex]\n",
    "\t\t\t\tstrPOS = dict_ontonotes[str_file][str_sent_index]['pos'][nTokenIndex]\n",
    "\t\t\t\tne_type = None\n",
    "\t\t\t\tif 'ne' in dict_ontonotes[str_file][str_sent_index] :\n",
    "\t\t\t\t\tdict_ne = dict_ontonotes[str_file][str_sent_index]['ne']\n",
    "\t\t\t\t\tif not 'parse_error' in dict_ne :\n",
    "\t\t\t\t\t\tfor str_NEIndex in dict_ne :\n",
    "\t\t\t\t\t\t\tif nTokenIndex in dict_ne[str_NEIndex]['tokens'] :\n",
    "\t\t\t\t\t\t\t\tne_type = dict_ne[str_NEIndex]['type']\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif ne_type != None :\n",
    "\t\t\t\t\tif ne_type == ne_type_last :\n",
    "\t\t\t\t\t\tstrIOB = 'I-' + ne_type\n",
    "\t\t\t\t\telse :\n",
    "\t\t\t\t\t\tstrIOB = 'B-' + ne_type\n",
    "\t\t\t\telse :\n",
    "\t\t\t\t\tstrIOB = 'O'\n",
    "\t\t\t\tne_type_last = ne_type\n",
    "\n",
    "\t\t\t\tlist_entry.append( ( strToken, strPOS, strIOB ) )\n",
    "\n",
    "\t\t\tlist_train.append( list_entry )\n",
    "\n",
    "\tlist_test = []\n",
    "\tfor str_file in list_test_files :\n",
    "\t\tfor str_sent_index in dict_ontonotes[str_file] :\n",
    "\t\t\t# ignore sents with non-PENN POS tags\n",
    "\t\t\tif 'XX' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif 'VERB' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tlist_entry = []\n",
    "\n",
    "\t\t\t# compute IOB tags for named entities (if any)\n",
    "\t\t\tne_type_last = None\n",
    "\t\t\tfor nTokenIndex in range(len(dict_ontonotes[str_file][str_sent_index]['tokens'])) :\n",
    "\t\t\t\tstrToken = dict_ontonotes[str_file][str_sent_index]['tokens'][nTokenIndex]\n",
    "\t\t\t\tstrPOS = dict_ontonotes[str_file][str_sent_index]['pos'][nTokenIndex]\n",
    "\t\t\t\tne_type = None\n",
    "\t\t\t\tif 'ne' in dict_ontonotes[str_file][str_sent_index] :\n",
    "\t\t\t\t\tdict_ne = dict_ontonotes[str_file][str_sent_index]['ne']\n",
    "\t\t\t\t\tif not 'parse_error' in dict_ne :\n",
    "\t\t\t\t\t\tfor str_NEIndex in dict_ne :\n",
    "\t\t\t\t\t\t\tif nTokenIndex in dict_ne[str_NEIndex]['tokens'] :\n",
    "\t\t\t\t\t\t\t\tne_type = dict_ne[str_NEIndex]['type']\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif ne_type != None :\n",
    "\t\t\t\t\tif ne_type == ne_type_last :\n",
    "\t\t\t\t\t\tstrIOB = 'I-' + ne_type\n",
    "\t\t\t\t\telse :\n",
    "\t\t\t\t\t\tstrIOB = 'B-' + ne_type\n",
    "\t\t\t\telse :\n",
    "\t\t\t\t\tstrIOB = 'O'\n",
    "\t\t\t\tne_type_last = ne_type\n",
    "\n",
    "\t\t\t\tlist_entry.append( ( strToken, strPOS, strIOB ) )\n",
    "\n",
    "\t\t\tlist_test.append( list_entry )\n",
    "\n",
    "\treturn list_train, list_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "556ca9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_word2features(sent, i):\n",
    "\t#print(sent)\n",
    "\tword = sent[i][0]\n",
    "\tpostag = sent[i][1]\n",
    "\n",
    "\tfeatures = {\n",
    "\t\t'word' : word,\n",
    "\t\t'postag': postag,\n",
    "\n",
    "\t\t# token shape\n",
    "\t\t'word.lower()': word.lower(),\n",
    "\t\t'word.isupper()': word.isupper(),\n",
    "\t\t'word.istitle()': word.istitle(),\n",
    "\t\t'word.isdigit()': word.isdigit(),\n",
    "\n",
    "\t\t# token suffix\n",
    "\t\t'word.suffix': word.lower()[-3:],\n",
    "\n",
    "\t\t# POS prefix\n",
    "\t\t'postag[:2]': postag[:2],\n",
    "\t}\n",
    "\tif i > 0:\n",
    "\t\tword_prev = sent[i-1][0]\n",
    "\t\tpostag_prev = sent[i-1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'-1:word.lower()': word_prev.lower(),\n",
    "\t\t\t'-1:postag': postag_prev,\n",
    "\t\t\t'-1:word.lower()': word_prev.lower(),\n",
    "\t\t\t'-1:word.isupper()': word_prev.isupper(),\n",
    "\t\t\t'-1:word.istitle()': word_prev.istitle(),\n",
    "\t\t\t'-1:word.isdigit()': word_prev.isdigit(),\n",
    "\t\t\t'-1:word.suffix': word_prev.lower()[-3:],\n",
    "\t\t\t'-1:postag[:2]': postag_prev[:2],\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['BOS'] = True\n",
    "\n",
    "\tif i < len(sent)-1:\n",
    "\t\tword_next = sent[i+1][0]\n",
    "\t\tpostag_next = sent[i+1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'+1:word.lower()': word_next.lower(),\n",
    "\t\t\t'+1:postag': postag_next,\n",
    "\t\t\t'+1:word.lower()': word_next.lower(),\n",
    "\t\t\t'+1:word.isupper()': word_next.isupper(),\n",
    "\t\t\t'+1:word.istitle()': word_next.istitle(),\n",
    "\t\t\t'+1:word.isdigit()': word_next.isdigit(),\n",
    "\t\t\t'+1:word.suffix': word_next.lower()[-3:],\n",
    "\t\t\t'+1:postag[:2]': postag_next[:2],\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['EOS'] = True\n",
    "\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d489c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task5_train_crf_model( X_train, Y_train, max_iter, labels ) :\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "      algorithm='lbfgs',\n",
    "      max_iterations=100,\n",
    "      all_possible_transitions=True\n",
    "  )\n",
    "    params_space = {\n",
    "      'c1': scipy.stats.expon(scale=0.5),\n",
    "      'c2': scipy.stats.expon(scale=0.05),\n",
    "  }\n",
    "\n",
    "  # use the same metric for evaluation\n",
    "    f1_scorer = make_scorer(sklearn_crfsuite.metrics.flat_f1_score,\n",
    "                          average='weighted', labels=labels)\n",
    "\n",
    "  # search\n",
    "    rs = sklearn.model_selection.RandomizedSearchCV(crf, params_space,\n",
    "                          cv=3,\n",
    "                          verbose=1,\n",
    "                          n_jobs=-1,\n",
    "                          n_iter=10,\n",
    "                          scoring=f1_scorer)\n",
    "    rs.fit(X_train, Y_train)\n",
    "    print('best params:', rs.best_params_)\n",
    "    print('best CV score:', rs.best_score_)\n",
    "    print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c2ec53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_book' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-696ed19b7ab8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexec_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_book\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0montonotes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2features_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask2_word2features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_crf_model_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask5_train_crf_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'file_book' is not defined"
     ]
    }
   ],
   "source": [
    "exec_task(file_book, ontonotes, word2features_func = task2_word2features,train_crf_model_func = task5_train_crf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf9acbff",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-45-aa6405c6ee26>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-45-aa6405c6ee26>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    best params: {'c1': 0.22341151833692405, 'c2': 0.061981414682358016}\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "best params: {'c1': 0.22341151833692405, 'c2': 0.061981414682358016}\n",
    "    best params: {'c1': 0.005782789658969212, 'c2': 0.17842452852345164}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8451f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
