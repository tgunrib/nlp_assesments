{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e610b059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tosin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sys, codecs, json, math, time, warnings\n",
    "import sklearn_crfsuite\n",
    "import sklearn\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "warnings.simplefilter( action='ignore', category=FutureWarning )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0395ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset( dataset_file, max_files = 50 ) :\n",
    "    # load parsed ontonotes dataset\n",
    "    readHandle = codecs.open( dataset_file, 'r', 'utf-8', errors = 'replace' )\n",
    "    str_json = readHandle.read()\n",
    "    readHandle.close()\n",
    "    dict_ontonotes = json.loads( str_json )\n",
    "\n",
    "    # make a training and test split\n",
    "    list_files = list( dict_ontonotes.keys() )\n",
    "    if len(list_files) > max_files :\n",
    "        list_files = list_files[ :max_files ]\n",
    "    nSplit = math.floor( len(list_files)*0.99 )\n",
    "    list_train_files = list_files[ : nSplit ]\n",
    "    list_test_files = list_files[ nSplit : ]\n",
    "\n",
    "    # sent = (tokens, pos, IOB_label)\n",
    "    list_train = []\n",
    "    for str_file in list_train_files :\n",
    "        for str_sent_index in dict_ontonotes[str_file] :\n",
    "            # ignore sents with non-PENN POS tags\n",
    "            if 'XX' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "                continue\n",
    "            if 'VERB' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "                continue\n",
    "\n",
    "            list_entry = []\n",
    "\n",
    "            # compute IOB tags for named entities (if any)\n",
    "            ne_type_last = None\n",
    "            for nTokenIndex in range(len(dict_ontonotes[str_file][str_sent_index]['tokens'])) :\n",
    "                strToken = dict_ontonotes[str_file][str_sent_index]['tokens'][nTokenIndex]\n",
    "                strPOS = dict_ontonotes[str_file][str_sent_index]['pos'][nTokenIndex]\n",
    "                ne_type = None\n",
    "                if 'ne' in dict_ontonotes[str_file][str_sent_index] :\n",
    "                    dict_ne = dict_ontonotes[str_file][str_sent_index]['ne']\n",
    "                    if not 'parse_error' in dict_ne :\n",
    "                        for str_NEIndex in dict_ne :\n",
    "                            if nTokenIndex in dict_ne[str_NEIndex]['tokens'] :\n",
    "                                ne_type = dict_ne[str_NEIndex]['type']\n",
    "                                break\n",
    "                if ne_type != None :\n",
    "                    if ne_type == ne_type_last :\n",
    "                        strIOB = 'I-' + ne_type\n",
    "                    else :\n",
    "                        strIOB = 'B-' + ne_type\n",
    "                else :\n",
    "                    strIOB = 'O'\n",
    "                ne_type_last = ne_type\n",
    "                \n",
    "                if  strIOB != 'I-PERSON' and strIOB != 'B-PERSON':\n",
    "                    strIOB = 'O'\n",
    "\n",
    "                list_entry.append( ( strToken, strPOS, strIOB ) )\n",
    "\n",
    "            list_train.append( list_entry )\n",
    "\n",
    "    list_test = []\n",
    "    for str_file in list_test_files :\n",
    "        for str_sent_index in dict_ontonotes[str_file] :\n",
    "\t\t\t# ignore sents with non-PENN POS tags\n",
    "            if 'XX' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "                continue\n",
    "            if 'VERB' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "                continue\n",
    "\n",
    "            list_entry = []\n",
    "\n",
    "            #compute IOB tags for named entities (if any)\n",
    "            ne_type_last = None\n",
    "            for nTokenIndex in range(len(dict_ontonotes[str_file][str_sent_index]['tokens'])) :\n",
    "                strToken = dict_ontonotes[str_file][str_sent_index]['tokens'][nTokenIndex]\n",
    "                strPOS = dict_ontonotes[str_file][str_sent_index]['pos'][nTokenIndex]\n",
    "                ne_type = None\n",
    "                if 'ne' in dict_ontonotes[str_file][str_sent_index] :\n",
    "                    dict_ne = dict_ontonotes[str_file][str_sent_index]['ne']\n",
    "                    if not 'parse_error' in dict_ne :\n",
    "                        for str_NEIndex in dict_ne :\n",
    "                            if nTokenIndex in dict_ne[str_NEIndex]['tokens'] :\n",
    "                                ne_type = dict_ne[str_NEIndex]['type']\n",
    "                                break\n",
    "                if ne_type != None:\n",
    "                    if ne_type == ne_type_last :\n",
    "                        strIOB = 'I-' + ne_type\n",
    "                    else :\n",
    "                        strIOB = 'B-' + ne_type\n",
    "                else :\n",
    "                    strIOB = 'O'\n",
    "                ne_type_last = ne_type\n",
    "\n",
    "                if  strIOB != 'I-PERSON' and strIOB != 'B-PERSON':\n",
    "                    strIOB = 'O'\n",
    "                    list_entry.append( ( strToken, strPOS, strIOB ) )\n",
    "            list_test.append( list_entry )\n",
    "        return list_train, list_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d52beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def names(check):\n",
    "    names_possibility = \"((?:Dr|Mr|Mrs|Miss|Sir|Lord|Lady|King|Professor|Doctor|Madam|Gentleman|Dame)\\.?\\s*(?:[A-Z]\\.?\\s?)*(?:[A-Z][a-z0-9\\-\\.]+\\s?)+|(?:[A-Z]\\.?\\s?)+(?:[A-Z][a-z0-9\\-\\.]+\\s?)+)\"\n",
    "    result = re.findall(names_possibility, check)\n",
    "    result = [re.sub('\\r|\\n', '', word.lower()) for word in result]\n",
    "    return result\n",
    "\n",
    "def names_match(text):\n",
    "    names_possibility =  \"((?:Dr|Mr|Mrs|Miss|Sir|Lord|Lady|King|Professor|Doctor|Madam|Gentleman|Dame)\\.?\\s*(?:[A-Z]\\.?\\s?)*(?:[A-Z][a-z0-9\\-,\\.]+\\s?)+|(?:[A-Z]\\.?\\s?)+(?:[A-Z][a-z0-9\\-,\\.]+\\s?)+)\"\n",
    "    return re.findall(names_possibility, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086f694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_textfile(filename):\n",
    "    text = ''\n",
    "    for line in codecs.open(filename, \"r\", encoding=\"utf-8\"):\n",
    "        text += line\n",
    "    sents = names(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_pos_tags = [nltk.pos_tag(nltk.word_tokenize(word)) for word in sentences]\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    # stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    word_lemmas = []\n",
    "    for sent in word_pos_tags:\n",
    "        # word_lemmas.append([(stemmer.stem(word[0]), word[1]) for word in sent])\n",
    "        word_lemmas.append([(lemmatizer.lemmatize(word[0]), word[1]) for word in sent])\n",
    "    return word_pos_tags, sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8beafbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_word2features(sent, i):\n",
    "\tword = sent[i][0]\n",
    "\tpostag = sent[i][1]\n",
    "\n",
    "\tfeatures = {\n",
    "\t\t'word' : word,\n",
    "\t\t'postag': postag,\n",
    "\n",
    "\t\t# token shape\n",
    "\t\t'word.lower()': word.lower(),\n",
    "\t\t'word.isupper()': word.isupper(),\n",
    "\t\t'word.istitle()': word.istitle(),\n",
    "\t\t'word.isdigit()': word.isdigit(),\n",
    "        'word.regex()' : names_match(word),\n",
    "\t\t# token suffix\n",
    "\t\t'word.suffix': word.lower()[-3:],\n",
    "\n",
    "\t\t# POS prefix\n",
    "\t\t'postag[:2]': postag[:2],\n",
    "\t}\n",
    "\tif i > 0:\n",
    "\t\tword_prev = sent[i-1][0]\n",
    "\t\tpostag_prev = sent[i-1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'-1:word.lower()': word_prev.lower(),\n",
    "\t\t\t'-1:postag': postag_prev,\n",
    "\t\t\t'-1:word.lower()': word_prev.lower(),\n",
    "\t\t\t'-1:word.isupper()': word_prev.isupper(),\n",
    "\t\t\t'-1:word.istitle()': word_prev.istitle(),\n",
    "\t\t\t'-1:word.isdigit()': word_prev.isdigit(),\n",
    "\t\t\t'-1:word.suffix': word_prev.lower()[-3:],\n",
    "\t\t\t'-1:postag[:2]': postag_prev[:2],\n",
    "            '-1:word.regex()' : names_match(word_prev),\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['BOS'] = True\n",
    "\n",
    "\tif i < len(sent)-1:\n",
    "\t\tword_next = sent[i+1][0]\n",
    "\t\tpostag_next = sent[i+1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'+1:word.lower()': word_next.lower(),\n",
    "\t\t\t'+1:postag': postag_next,\n",
    "\t\t\t'+1:word.lower()': word_next.lower(),\n",
    "\t\t\t'+1:word.isupper()': word_next.isupper(),\n",
    "\t\t\t'+1:word.istitle()': word_next.istitle(),\n",
    "\t\t\t'+1:word.isdigit()': word_next.isdigit(),\n",
    "\t\t\t'+1:word.suffix': word_next.lower()[-3:],\n",
    "\t\t\t'+1:postag[:2]': postag_next[:2],\n",
    "            '+1:word.regex()' : names_match(word_next),\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['EOS'] = True\n",
    "\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37797c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent, word2features_func = None):\n",
    "\treturn [word2features_func(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "\treturn [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "\treturn [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6c3b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_task(filebook, dataset_file, word2features_func, max_files = 20,train_crf_model_func = None,  max_iter = 50) :\n",
    "    #make a dataset from english NE labelled ontonotes sents\n",
    "    train_sents, test_sents = create_dataset( dataset_file, max_files = max_files )\n",
    "    txt_sents, sentences = preprocess_textfile(filebook)\n",
    "    \n",
    "   \n",
    "    sent = names(text)\n",
    "    word_pos_tags = [nltk.pos_tag(nltk.word_tokenize(word)) for word in sent]\n",
    "    \n",
    "\n",
    "    # create feature vectors for every sent\n",
    "    X_train = [sent2features(s, word2features_func = word2features_func) for s in train_sents]\n",
    "    Y_train = [sent2labels(s) for s in train_sents]\n",
    "    \n",
    "    unsup_text = [sent2features(s, word2features_func = word2features_func) for s in txt_sents]\n",
    "    #name_text = [sent2features(s, word2features_func = word2features_func) for s in word_pos_tags]\n",
    "\n",
    "    X_test = [sent2features(s, word2features_func = word2features_func) for s in test_sents]\n",
    "    Y_test = [sent2labels(s) for s in test_sents]\n",
    "    \n",
    "\n",
    "    # get the label set\n",
    "    set_labels = set([])\n",
    "    for data in [Y_train,Y_test] :\n",
    "        for n_sent in range(len(data)) :\n",
    "            for str_label in data[n_sent] :\n",
    "                set_labels.add( str_label )\n",
    "    labels = list( set_labels )\n",
    "\n",
    "    # remove 'O' label as we are not usually interested in how well 'O' is predicted\n",
    "    #labels = list( crf.classes_ )\n",
    "    labels.remove('O')\n",
    "    \n",
    "    \n",
    "    crf = train_crf_model_func( X_train, Y_train, max_iter, labels)\n",
    "    Y_pred = crf.predict( X_test )\n",
    "    #names_pred = crf.predict(name_text)\n",
    "    result_pred = crf.predict(unsup_text)\n",
    "    #print(len(Y_pred[1]), len(X_test[1]))\n",
    "    sorted_labels = sorted(\n",
    "        labels, \n",
    "        key=lambda name: (name[1:], name[0])\n",
    "    )\n",
    "     \n",
    "    macro_scores = sklearn_crfsuite.metrics.flat_classification_report( Y_test, Y_pred, labels=sorted_labels)\n",
    "    print( macro_scores )\n",
    "    result = []\n",
    "    accept = ['PERSON']\n",
    "    for i in range(0,len(result_pred)):\n",
    "        conlltags = [(word['word'], word['postag'] , tg) for tg, word in zip(result_pred[i], unsup_text[i])]\n",
    "        ne_tree = nltk.chunk.conlltags2tree(conlltags)\n",
    "        for subtree in ne_tree:\n",
    "            if type(subtree) == nltk.tree.Tree:\n",
    "                original_label = subtree.label()\n",
    "                original_string = \" \".join([token for token, pos in subtree.leaves()]).lower().strip()\n",
    "                result.append((original_string, original_label))\n",
    "    \"\"\"for i in range(0,len(names_pred)):\n",
    "        conlltags = [(word['word'], word['postag'] , tg) for tg, word in zip(names_pred[i], name_text[i])]\n",
    "        ne_tree = nltk.chunk.conlltags2tree(conlltags)\n",
    "        for subtree in ne_tree:\n",
    "            if type(subtree) == nltk.tree.Tree:\n",
    "                original_label = subtree.label()\n",
    "                original_string = \" \".join([token for token, pos in subtree.leaves()]).lower().strip()\n",
    "                result.append((original_string, original_label))\"\"\"\n",
    "    d = []\n",
    "    for value,key in result:\n",
    "        if value not in result:\n",
    "            d.append(value.strip())\n",
    "    d = d + sentences\n",
    "    return set(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b54fb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn_crfsuite import metrics\n",
    "def task5_train_crf_model( X_train, Y_train, max_iter, labels ) :\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "      algorithm='lbfgs',\n",
    "      max_iterations=100,\n",
    "      all_possible_transitions=True\n",
    "  )\n",
    "    params_space = {\n",
    "      'c1': scipy.stats.expon(scale=0.5),\n",
    "      'c2': scipy.stats.expon(scale=0.05),\n",
    "  }\n",
    "\n",
    "  # use the same metric for evaluation\n",
    "    f1_scorer = make_scorer(sklearn_crfsuite.metrics.flat_f1_score,\n",
    "                          average='weighted', labels=labels)\n",
    "\n",
    "  # search\n",
    "    rs = sklearn.model_selection.RandomizedSearchCV(crf, params_space,\n",
    "                          cv=3,\n",
    "                          verbose=1,\n",
    "                          n_jobs=-1,\n",
    "                          n_iter=10,\n",
    "                          scoring=f1_scorer)\n",
    "    rs.fit(X_train, Y_train)\n",
    "    print('best params:', rs.best_params_)\n",
    "    print('best CV score:', rs.best_score_)\n",
    "    print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f25038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task3_train_crf_model( X_train, Y_train, max_iter, labels ) :\n",
    "\t# train the basic CRF model\n",
    "\tcrf = sklearn_crfsuite.CRF(\n",
    "\t\talgorithm='lbfgs',\n",
    "\t\tc1=0.1,\n",
    "\t\tc2=0.1,\n",
    "\t\tmax_iterations=max_iter,\n",
    "\t\tall_possible_transitions=False,\n",
    "\t)\n",
    "\tcrf.fit(X_train, Y_train)\n",
    "\treturn crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "150034a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-10-c23e22ab5088>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mfile_book\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'C:\\\\Users\\\\tosin\\\\Documents\\\\Comp3225_coursework\\\\comp3225_example_package\\\\eval_chapter.txt'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0montonotes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'C:\\\\Users\\\\tosin\\\\Documents\\\\Comp3225_coursework\\\\comp3225_example_package\\\\ontonotes_parsed.json'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mexec_task\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile_book\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0montonotes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mword2features_func\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtask2_word2features\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtrain_crf_model_func\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtask5_train_crf_model\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-7-855d251b5bb7>\u001B[0m in \u001B[0;36mexec_task\u001B[1;34m(filebook, dataset_file, word2features_func, max_files, train_crf_model_func, max_iter)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m     \u001B[0msent\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m     \u001B[0mword_pos_tags\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpos_tag\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mword_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msent\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "file_book = 'C:\\\\Users\\\\tosin\\\\Documents\\\\Comp3225_coursework\\\\comp3225_example_package\\\\eval_chapter.txt'\n",
    "ontonotes = 'C:\\\\Users\\\\tosin\\\\Documents\\\\Comp3225_coursework\\\\comp3225_example_package\\\\ontonotes_parsed.json'\n",
    "exec_task(file_book, ontonotes, word2features_func = task2_word2features,train_crf_model_func = task5_train_crf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f3c046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
