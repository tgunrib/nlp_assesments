{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2023-05-03 16:40:28,308 logging started\n"
     ]
    }
   ],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "######################################################################\n",
    "#\n",
    "# (c) Copyright University of Southampton, 2021\n",
    "#\n",
    "# Copyright in this software belongs to University of Southampton,\n",
    "# Highfield, University Road, Southampton SO17 1BJ\n",
    "#\n",
    "# Created By : Stuart E. Middleton\n",
    "# Created Date : 2021/01/29\n",
    "# Project : Teaching\n",
    "#\n",
    "######################################################################\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import sys, codecs, json, math, time, warnings, re, logging\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import nltk, numpy, scipy, sklearn, sklearn_crfsuite, sklearn_crfsuite.metrics\n",
    "\n",
    "LOG_FORMAT = ('%(levelname) -s %(asctime)s %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)\n",
    "logger.info('logging started')\n",
    "\n",
    "def preprocess_textfile(filename):\n",
    "\ttext = ''\n",
    "\tfor line in codecs.open(filename, \"r\", encoding=\"utf-8\"):\n",
    "\t\ttext += line\n",
    "\tsentences = nltk.sent_tokenize(text)\n",
    "\tword_pos_tags = [nltk.pos_tag(nltk.word_tokenize(word)) for word in sentences]\n",
    "\tlemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\tword_lemmas = []\n",
    "\tfor sent in word_pos_tags:\n",
    "\t\tword_lemmas.append([(lemmatizer.lemmatize(word[0]), word[1]) for word in sent])\n",
    "\treturn word_pos_tags, word_pos_tags\n",
    "\n",
    "\n",
    "def create_dataset(dataset_file, max_files=None):\n",
    "\t# load parsed ontonotes dataset\n",
    "\treadHandle = codecs.open(dataset_file, 'r', 'utf-8', errors='replace')\n",
    "\tstr_json = readHandle.read()\n",
    "\treadHandle.close()\n",
    "\tdict_ontonotes = json.loads(str_json)\n",
    "\n",
    "\t# make a training and test split\n",
    "\tlist_files = list(dict_ontonotes.keys())\n",
    "\tif len(list_files) > max_files:\n",
    "\t\tlist_files = list_files[:max_files]\n",
    "\tnSplit = math.floor(len(list_files) * 0.95)\n",
    "\tlist_train_files = list_files[: nSplit]\n",
    "\tlist_test_files = list_files[nSplit:]\n",
    "\n",
    "\t# sent = (tokens, pos, IOB_label)\n",
    "\tlist_train = []\n",
    "\tfor str_file in list_train_files:\n",
    "\t\tfor str_sent_index in dict_ontonotes[str_file]:\n",
    "\t\t\t# ignore sents with non-PENN POS tags\n",
    "\t\t\tif 'XX' in dict_ontonotes[str_file][str_sent_index]['pos']:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif 'VERB' in dict_ontonotes[str_file][str_sent_index]['pos']:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tlist_entry = []\n",
    "\n",
    "\t\t\t# compute IOB tags for named entities (if any)\n",
    "\t\t\tne_type_last = None\n",
    "\t\t\tfor nTokenIndex in range(len(dict_ontonotes[str_file][str_sent_index]['tokens'])):\n",
    "\t\t\t\tstrToken = dict_ontonotes[str_file][str_sent_index]['tokens'][nTokenIndex]\n",
    "\t\t\t\tstrPOS = dict_ontonotes[str_file][str_sent_index]['pos'][nTokenIndex]\n",
    "\t\t\t\tne_type = None\n",
    "\t\t\t\tif 'ne' in dict_ontonotes[str_file][str_sent_index]:\n",
    "\t\t\t\t\tdict_ne = dict_ontonotes[str_file][str_sent_index]['ne']\n",
    "\t\t\t\t\tif not 'parse_error' in dict_ne:\n",
    "\t\t\t\t\t\tfor str_NEIndex in dict_ne:\n",
    "\t\t\t\t\t\t\tif nTokenIndex in dict_ne[str_NEIndex]['tokens']:\n",
    "\t\t\t\t\t\t\t\tne_type = dict_ne[str_NEIndex]['type']\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif ne_type == 'DATE' or ne_type == 'ORDINAL' or ne_type == 'NORP' or ne_type == 'CARDINAL':\n",
    "\t\t\t\t\tif ne_type == ne_type_last:\n",
    "\t\t\t\t\t\tstrIOB = 'I-' + ne_type\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tstrIOB = 'B-' + ne_type\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tstrIOB = 'O'\n",
    "\t\t\t\tne_type_last = ne_type\n",
    "\n",
    "\t\t\t\tlist_entry.append((strToken, strPOS, strIOB))\n",
    "\n",
    "\t\t\tlist_train.append(list_entry)\n",
    "\n",
    "\tlist_test = []\n",
    "\tfor str_file in list_test_files:\n",
    "\t\tfor str_sent_index in dict_ontonotes[str_file]:\n",
    "\t\t\t# ignore sents with non-PENN POS tags\n",
    "\t\t\tif 'XX' in dict_ontonotes[str_file][str_sent_index]['pos']:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif 'VERB' in dict_ontonotes[str_file][str_sent_index]['pos']:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tlist_entry = []\n",
    "\n",
    "\t\t\t# compute IOB tags for named entities (if any)\n",
    "\t\t\tne_type_last = None\n",
    "\t\t\tfor nTokenIndex in range(len(dict_ontonotes[str_file][str_sent_index]['tokens'])):\n",
    "\t\t\t\tstrToken = dict_ontonotes[str_file][str_sent_index]['tokens'][nTokenIndex]\n",
    "\t\t\t\tstrPOS = dict_ontonotes[str_file][str_sent_index]['pos'][nTokenIndex]\n",
    "\t\t\t\tne_type = None\n",
    "\t\t\t\tif 'ne' in dict_ontonotes[str_file][str_sent_index]:\n",
    "\t\t\t\t\tdict_ne = dict_ontonotes[str_file][str_sent_index]['ne']\n",
    "\t\t\t\t\tif not 'parse_error' in dict_ne:\n",
    "\t\t\t\t\t\tfor str_NEIndex in dict_ne:\n",
    "\t\t\t\t\t\t\tif nTokenIndex in dict_ne[str_NEIndex]['tokens']:\n",
    "\t\t\t\t\t\t\t\tne_type = dict_ne[str_NEIndex]['type']\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif ne_type == 'DATE' or ne_type == 'ORDINAL' or ne_type == 'NORP' or ne_type == 'CARDINAL':\n",
    "\t\t\t\t\tif ne_type == ne_type_last:\n",
    "\t\t\t\t\t\tstrIOB = 'I-' + ne_type\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tstrIOB = 'B-' + ne_type\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tstrIOB = 'O'\n",
    "\t\t\t\tne_type_last = ne_type\n",
    "\n",
    "\t\t\t\tlist_entry.append((strToken, strPOS, strIOB))\n",
    "\n",
    "\t\t\tlist_test.append(list_entry)\n",
    "\n",
    "\treturn list_train, list_test\n",
    "\n",
    "\n",
    "def task2_word2features(sent, i):\n",
    "\tword = sent[i][0]\n",
    "\tpostag = sent[i][1]\n",
    "\n",
    "\tfeatures = {\n",
    "\t\t'word': word,\n",
    "\t\t'postag': postag,\n",
    "\n",
    "\t\t# token shape\n",
    "\t\t'word.lower()': word.lower(),\n",
    "\t\t'word.isupper()': word.isupper(),\n",
    "\t\t'word.istitle()': word.istitle(),\n",
    "\t\t'word.isdigit()': word.isdigit(),\n",
    "\n",
    "\t\t# token suffix\n",
    "\t\t'word.suffix': word.lower()[-3:],\n",
    "\n",
    "\t\t# POS prefix\n",
    "\t\t'postag[:2]': postag[:2],\n",
    "\t}\n",
    "\tif i > 0:\n",
    "\t\tword_prev = sent[i - 1][0]\n",
    "\t\tpostag_prev = sent[i - 1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'-1:word.lower()': word_prev.lower(),\n",
    "\t\t\t'-1:postag': postag_prev,\n",
    "\t\t\t'-1:word.isupper()': word_prev.isupper(),\n",
    "\t\t\t'-1:word.istitle()': word_prev.istitle(),\n",
    "\t\t\t'-1:word.isdigit()': word_prev.isdigit(),\n",
    "\t\t\t'-1:word.suffix': word_prev.lower()[-3:],\n",
    "\t\t\t'-1:postag[:2]': postag_prev[:2],\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['BOS'] = True\n",
    "\n",
    "\tif i < len(sent) - 1:\n",
    "\t\tword_next = sent[i + 1][0]\n",
    "\t\tpostag_next = sent[i + 1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'+1:word.lower()': word_next.lower(),\n",
    "\t\t\t'+1:postag': postag_next,\n",
    "\t\t\t'+1:word.isupper()': word_next.isupper(),\n",
    "\t\t\t'+1:word.istitle()': word_next.istitle(),\n",
    "\t\t\t'+1:word.isdigit()': word_next.isdigit(),\n",
    "\t\t\t'+1:word.suffix': word_next.lower()[-3:],\n",
    "\t\t\t'+1:postag[:2]': postag_next[:2],\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['EOS'] = True\n",
    "\n",
    "\treturn features\n",
    "\n",
    "\n",
    "def sent2features(sent, word2features_func=None):\n",
    "\treturn [word2features_func(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "\treturn [label for token, postag, label in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "\treturn [token for token, postag, label in sent]\n",
    "\n",
    "\n",
    "def exec_task(filebook, dataset_file, word2features_func, max_files=3500, train_crf_model_func=None, max_iter=100):\n",
    "\t# make a dataset from english NE labelled ontonotes sents\n",
    "\ttrain_sents, test_sents = create_dataset(dataset_file, max_files=max_files)\n",
    "\ttxt_sents, sentences = preprocess_textfile(filebook)\n",
    "\n",
    "\t# create feature vectors for every sent\n",
    "\tX_train = [sent2features(s, word2features_func=word2features_func) for s in train_sents]\n",
    "\tY_train = [sent2labels(s) for s in train_sents]\n",
    "\tunsup_text = [sent2features(s, word2features_func=word2features_func) for s in txt_sents]\n",
    "\n",
    "\tX_test = [sent2features(s, word2features_func=word2features_func) for s in test_sents]\n",
    "\tY_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "\t# get the label set\n",
    "\tset_labels = set([])\n",
    "\tfor data in [Y_train, Y_test]:\n",
    "\t\tfor n_sent in range(len(data)):\n",
    "\t\t\tfor str_label in data[n_sent]:\n",
    "\t\t\t\tset_labels.add(str_label)\n",
    "\tlabels = list(set_labels)\n",
    "\n",
    "\t# remove 'O' label as we are not usually interested in how well 'O' is predicted\n",
    "\t# labels = list( crf.classes_ )\n",
    "\tlabels.remove('O')\n",
    "\n",
    "\tcrf = train_crf_model_func(X_train, Y_train, max_iter, labels)\n",
    "\tY_pred = crf.predict(X_test)\n",
    "\tresult_pred = crf.predict(unsup_text)\n",
    "\t# print(len(Y_pred[1]), len(X_test[1]))\n",
    "\tsorted_labels = sorted(\n",
    "\t\tlabels,\n",
    "\t\tkey=lambda name: (name[1:], name[0])\n",
    "\t)\n",
    "\n",
    "\t# macro_scores = sklearn_crfsuite.metrics.flat_classification_report( Y_test, Y_pred, labels=sorted_labels)\n",
    "\t# print( macro_scores )\n",
    "\tresult = []\n",
    "\t#print(len(X_test[1]), len(Y_pred[1]))\n",
    "\tfor i in range(0, len(result_pred)):\n",
    "\t\tconlltags = [(word['word'], word['postag'], tg) for tg, word in zip(result_pred[i], unsup_text[i])]\n",
    "\t\tne_tree = nltk.chunk.conlltags2tree(conlltags)\n",
    "\t\tfor subtree in ne_tree:\n",
    "\t\t\tif type(subtree) == nltk.tree.Tree:\n",
    "\t\t\t\toriginal_label = subtree.label()\n",
    "\t\t\t\toriginal_string = \" \".join([token for token, pos in subtree.leaves()]).lower().strip()\n",
    "\t\t\t\tresult.append((original_string, original_label))\n",
    "\td = {}\n",
    "\tfor value, key in result:\n",
    "\t\tif key in d and value not in d[key] or key not in d:\n",
    "\t\t\td.setdefault(key, []).append(value)\n",
    "\treturn d\n",
    "\n",
    "\n",
    "def task3_train_crf_model(X_train, Y_train, max_iter, labels):\n",
    "\t# train the basic CRF model\n",
    "\tcrf = sklearn_crfsuite.CRF(\n",
    "\t\talgorithm='lbfgs',\n",
    "\t\tc1=0.005,\n",
    "\t\tc2=0.17,\n",
    "\t\tmax_iterations=max_iter,\n",
    "\t\tall_possible_transitions=False,\n",
    "\t)\n",
    "\tcrf.fit(X_train, Y_train)\n",
    "\treturn crf\n",
    "\n",
    "def exec_ner(file_chapter=None, ontonotes_file=None):\n",
    "\t# CHANGE CODE BELOW TO TRAIN A CRF NER MODEL TO TAG THE CHAPTER OF TEXT (task 3)\n",
    "\n",
    "\t# Input >> www.gutenberg.org sourced plain text file for a chapter of a book\n",
    "\t# Output >> ne.json = { <ne_type> : [ <phrase>, <phrase>, ... ] }\n",
    "\n",
    "\t# hardcoded output to show exactly what is expected to be serialized (you should change this)\n",
    "\t# only the allowed types for task 3 DATE, CARDINAL, ORDINAL, NORP will be serialized\n",
    "\tdictNE = exec_task(file_chapter, ontonotes_file, word2features_func=task2_word2features, train_crf_model_func=task3_train_crf_model)\n",
    "\n",
    "\t# DO NOT CHANGE THE BELOW CODE WHICH WILL SERIALIZE THE ANSWERS FOR THE AUTOMATED TEST HARNESS TO LOAD AND MARK\n",
    "\n",
    "\t# FILTER NE dict by types required for task 3\n",
    "\tlistAllowedTypes = ['DATE', 'CARDINAL', 'ORDINAL', 'NORP']\n",
    "\tlistKeys = list(dictNE.keys())\n",
    "\tfor strKey in listKeys:\n",
    "\t\tfor nIndex in range(len(dictNE[strKey])):\n",
    "\t\t\tdictNE[strKey][nIndex] = dictNE[strKey][nIndex].strip().lower()\n",
    "\t\tif not strKey in listAllowedTypes:\n",
    "\t\t\tdel dictNE[strKey]\n",
    "\n",
    "\t# write filtered NE dict\n",
    "\twriteHandle = codecs.open('ne.json', 'w', 'utf-8', errors='replace')\n",
    "\tstrJSON = json.dumps(dictNE, indent=2)\n",
    "\twriteHandle.write(strJSON + '\\n')\n",
    "\twriteHandle.close()\n",
    "\n",
    "\n",
    "\n",
    "ontonotes_file = 'C:\\\\Users\\\\tosin\\\\Documents\\\\Comp3225_coursework\\\\comp3225_example_package\\\\ontonotes_parsed.json'\n",
    "chapter_file = 'C:\\\\Users\\\\tosin\\\\Documents\\\\Comp3225_coursework\\\\comp3225_example_package\\\\eval_chapter.txt'\n",
    "\n",
    "#logger.info('ontonotes = ' + repr(ontonotes_file))\n",
    "#logger.info('book = ' + repr(book_file))\n",
    "#logger.info('chapter = ' + repr(chapter_file))\n",
    "\n",
    "# DO NOT CHANGE THE CODE IN THIS FUNCTION\n",
    "\n",
    "exec_ner(chapter_file, ontonotes_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
